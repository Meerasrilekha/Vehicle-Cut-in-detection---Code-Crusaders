{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meerasrilekha/Vehicle-Cut-in-detection---Code-Crusaders/blob/main/Vehicle_cut_in_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoRn6s553r8t",
        "outputId": "f606179b-8bf6-4165-ffc1-0d2fff390e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ground_truth_csv.csv\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Moviepy - Building video result.mp4.\n",
            "MoviePy - Writing audio in resultTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video result.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready result.mp4\n"
          ]
        }
      ],
      "source": [
        "#After re-encoding the vehicle input video final_video_processing function processes the final cut-in detection video\n",
        "from moviepy.editor import VideoFileClip\n",
        "#function starts here with parameters of the input video file and csv file which contains vehicle dataset\n",
        "def final_video_processing(video_file, ground_truth_csv):\n",
        "  #path for the csv file\n",
        "    ground_truth_data = read_ground_truth_from_csv(ground_truth_csv)\n",
        "    video_clip = VideoFileClip(video_file)\n",
        "    #splits the video into different frames and evaluates each frame\n",
        "    frame_list = [frame for frame in video_clip.iter_frames()]\n",
        "    all_detected_vehicles = []\n",
        "    all_ground_truth_vehicles = []\n",
        "    #\n",
        "    for frame_number, frame in enumerate(frame_list):\n",
        "    # Iterate through each frame in the frame_list\n",
        "    # frame_number is the index of the current frame\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        # Convert the frame from RGB to BGR color space, as required by OpenCV\n",
        "        final_frame = pipeline(frame_rgb)\n",
        "        # Pass the BGR frame through a pipeline (likely a machine learning model) and to get the final processed frame\n",
        "        ground_truth_vehicles = []\n",
        "        for label in ground_truth_data:\n",
        "            ground_truth_vehicles.extend(ground_truth_data[label])\n",
        "        # Collect all the ground truth vehicle locations from the ground_truth_data\n",
        "        # The ground_truth_data is likely a dictionary, where the keys are labels and the values are lists of vehicle locations\n",
        "\n",
        "        detected_vehicles = detect_vehicles(final_frame)\n",
        "        # Use a vehicle detection algorithm to detect vehicles in the final_frame and the detected_vehicles is a list of detected vehicle locations\n",
        "\n",
        "        all_detected_vehicles.extend(detected_vehicles)\n",
        "        all_ground_truth_vehicles.extend(ground_truth_vehicles)\n",
        "    # Accumulate all the detected and ground truth vehicles across all frames\n",
        "\n",
        "    tp, fp, fn = evaluate_detection(all_detected_vehicles, all_ground_truth_vehicles)\n",
        "    # Evaluate the detection performance by comparing the detected vehicles\n",
        "    # true positives (tp), false positives (fp), and false negatives (fn)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    # Calculate the precision, which is the ratio of true positives to\n",
        "    # the sum of true positives and false positives\n",
        "    # If there are no true positives and false positives, the precision is 0\n",
        "\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    # Calculate the recall, which is the ratio of true positives to\n",
        "    # the sum of true positives and false negatives\n",
        "    # If there are no true positives and false negatives, the recall is 0\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    # Calculate the F1-score, which is the harmonic mean of precision and recall\n",
        "    # The F1-score is a balanced metric that considers both precision and recall\n",
        "    # If the sum of precision and recall is 0, the F1-score is also 0\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "    output_file = 'result.mp4'\n",
        "    final_clip = VideoFileClip(video_file).fl_image(lambda img: pipeline(cv2.cvtColor(img, cv2.COLOR_RGB2BGR)))\n",
        "    final_clip.write_videofile(output_file, codec='libx264')\n",
        "#function calling and final output processing\n",
        "if __name__ == \"__main__\":\n",
        "    vehicle_input_video = \"New_Project.mp4\"\n",
        "    vehicle_input_trail = \"New_Project.mp4\"\n",
        "    ground_truth_csv_path = \"ground_truth_csv.csv\"\n",
        "    reencode_video(vehicle_input_video,vehicle_input_trail)\n",
        "    print(ground_truth_csv_path)\n",
        "    final_video_processing(vehicle_input_trail, ground_truth_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufow7n8G3uSN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMy5qxQa3uer",
        "outputId": "bf26d4ad-2dee-42d7-dfbd-468b82a01074"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:293: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 5.1 MB/s eta 0:00:00\n",
            "Collecting pillow>=10.3.0\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 76.2 MB/s eta 0:00:00\n",
            "Collecting requests>=2.32.0\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 201.4 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 164.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, requests, pillow, gitdb, gitpython\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pillow-10.4.0 requests-2.32.3 smmap-5.0.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 12.3s, installed 3 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 2024-7-14 Python-3.10.12 torch-2.3.0+cu121 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100%|██████████| 14.1M/14.1M [00:00<00:00, 127MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import subprocess\n",
        "\n",
        "def reencode_video(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Reencode a video file using FFmpeg.\n",
        "    This function is used to convert the input video file to a format that can be easily processed.\n",
        "    \"\"\"\n",
        "    cmd = f\"ffmpeg -i {input_file} -c:v copy -c:a copy -movflags faststart {output_file}\"\n",
        "    subprocess.call(cmd, shell=True)\n",
        "\n",
        "def region_of_interest(img, vertices):\n",
        "    \"\"\"\n",
        "    Apply a mask to the input image to focus on a specific region of interest.\n",
        "    This function is used to select the region of the image where the vehicle detection will be performed.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(img)\n",
        "    match_mask_color = 255\n",
        "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
        "    masked_image = cv2.bitwise_and(img, mask)\n",
        "    return masked_image\n",
        "\n",
        "def draw_lines(img, lines, color=[255, 0, 0], thickness=3):\n",
        "    \"\"\"\n",
        "    Draw lines on the input image.\n",
        "    This function is used to visualize the detected lane lines on the image.\n",
        "    \"\"\"\n",
        "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "    img = np.copy(img)\n",
        "    if lines is None:\n",
        "        return img\n",
        "    for line in lines:\n",
        "        for x1, y1, x2, y2 in line:\n",
        "            cv2.line(line_img, (x1, y1), (x2, y2), color, thickness)\n",
        "    img = cv2.addWeighted(img, 0.8, line_img, 1.0, 0.0)\n",
        "    return img\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "def detect_vehicles(image):\n",
        "    \"\"\"\n",
        "    Detect vehicles in the input image using the YOLOv5 model.\n",
        "    This function returns a list of detected vehicles, where each vehicle is represented as a tuple (x, y, w, h).\n",
        "    \"\"\"\n",
        "    results = model(image)\n",
        "    detections = results.pandas().xyxy[0]\n",
        "\n",
        "    vehicles_detected = []\n",
        "    for _, row in detections.iterrows():\n",
        "        if row['name'] in [\"car\", \"bus\", \"truck\", \"motorcycle\", \"scooty\", \"bike\"]:\n",
        "            x, y, w, h = int(row['xmin']), int(row['ymin']), int(row['xmax']) - int(row['xmin']), int(row['ymax']) - int(row['ymin'])\n",
        "            vehicles_detected.append((x, y, w, h))\n",
        "            label = row['name']\n",
        "            color = (0, 0, 255)\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    return vehicles_detected\n",
        "\n",
        "def highlight_vehicle_area(image, left_line, right_line, vehicles_detected):\n",
        "    \"\"\"\n",
        "    Highlight the area where vehicles are detected and check for vehicle cut-in.\n",
        "    This function draws a green polygon around the detected vehicle area and displays a warning if a vehicle is detected cutting into the lane.\n",
        "    \"\"\"\n",
        "    overlay = image.copy()\n",
        "    warning = False\n",
        "    if left_line is not None and right_line is not None:\n",
        "        height = image.shape[0]\n",
        "        min_y = int(height * (3 / 5))\n",
        "        max_y = int(height)\n",
        "\n",
        "        left_x_start, left_y_start, left_x_end, left_y_end = left_line\n",
        "        right_x_start, right_y_start, right_x_end, right_y_end = right_line\n",
        "\n",
        "        pts = np.array([\n",
        "            [left_x_start, left_y_start],\n",
        "            [left_x_end, left_y_end],\n",
        "            [right_x_end, right_y_end],\n",
        "            [right_x_start, right_y_start]\n",
        "        ], np.int32)\n",
        "\n",
        "        cv2.fillPoly(overlay, [pts], (0, 255, 0))\n",
        "        cv2.addWeighted(overlay, 0.3, image, 0.7, 0, image)\n",
        "\n",
        "        for (x, y, w, h) in vehicles_detected:\n",
        "            vehicle_center = (float(x + w // 2), float(y + h // 2))\n",
        "            if cv2.pointPolygonTest(pts, vehicle_center, False) >= 0:\n",
        "                warning = True\n",
        "\n",
        "    if warning:\n",
        "        cv2.putText(image, \"Warning!!!!! Vehicle cut-in detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "def pipeline(image):\n",
        "    \"\"\"\n",
        "    The main processing pipeline for the vehicle cut-in detection.\n",
        "    This function takes an input image, processes it, and returns the final image with the detected vehicles and the warning for vehicle cut-in.\n",
        "    \"\"\"\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    region_of_interest_vertices = [\n",
        "        (0, height),\n",
        "        (width / 2, height / 2),\n",
        "        (width, height),\n",
        "    ]\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    cannyed_image = cv2.Canny(gray_image, 100, 200)\n",
        "\n",
        "    cropped_image = region_of_interest(cannyed_image, np.array([region_of_interest_vertices], np.int32))\n",
        "\n",
        "    lines = cv2.HoughLinesP(cropped_image, rho=6, theta=np.pi / 60, threshold=160, lines=np.array([]), minLineLength=20, maxLineGap=300)\n",
        "\n",
        "    left_line_x = []\n",
        "    left_line_y = []\n",
        "    right_line_x = []\n",
        "    right_line_y = []\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            for x1, y1, x2, y2 in line:\n",
        "                if x2 - x1 == 0:  # Prevent division by zero\n",
        "                    continue\n",
        "                slope = (y2 - y1) / (x2 - x1)\n",
        "                if math.fabs(slope) < 0.5:\n",
        "                    continue\n",
        "                if slope <= 0:\n",
        "                    left_line_x.extend([x1, x2])\n",
        "                    left_line_y.extend([y1, y2])\n",
        "                else:\n",
        "                    right_line_x.extend([x1, x2])\n",
        "                    right_line_y.extend([y1, y2])\n",
        "\n",
        "    left_line = None\n",
        "    right_line = None\n",
        "    if left_line_x and left_line_y and right_line_x and right_line_y:\n",
        "        min_y = int(image.shape[0] * (3 / 5))\n",
        "        max_y = int(image.shape[0])\n",
        "\n",
        "        if left_line_x and left_line_y:\n",
        "            poly_left = np.poly1d(np.polyfit(left_line_y, left_line_x, deg=1))\n",
        "            left_line = (int(poly_left(max_y)), max_y, int(poly_left(min_y)), min_y)\n",
        "\n",
        "        if right_line_x and right_line_y:\n",
        "            poly_right = np.poly1d(np.polyfit(right_line_y, right_line_x, deg=1))\n",
        "            right_line = (int(poly_right(max_y)), max_y, int(poly_right(min_y)), min_y)\n",
        "\n",
        "        line_image = draw_lines(image, [[left_line, right_line]], thickness=5)\n",
        "    else:\n",
        "        line_image = image  # If no lines are detected, return the original image\n",
        "\n",
        "    vehicles_detected = detect_vehicles(line_image)\n",
        "\n",
        "    final_image = highlight_vehicle_area(line_image, left_line, right_line, vehicles_detected)\n",
        "\n",
        "    return final_image\n",
        "\n",
        "def calculate_iou(boxA, boxB):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
        "    This function is used to evaluate the detection performance by comparing the detected vehicles with the ground truth vehicles.\n",
        "    \"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "    return iou\n",
        "\n",
        "def evaluate_detection(detected_vehicles, ground_truth_vehicles, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate the detection performance by comparing the detected vehicles with the ground truth vehicles.\n",
        "    This function returns the number of true positives, false positives, and false negatives.\n",
        "    \"\"\"\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    matched_ground_truths = []\n",
        "\n",
        "    for gt_vehicle in ground_truth_vehicles:\n",
        "        gt_box = (gt_vehicle[0], gt_vehicle[1], gt_vehicle[0] + gt_vehicle[2], gt_vehicle[1] + gt_vehicle[3])\n",
        "        matched = False\n",
        "        for det_vehicle in detected_vehicles:\n",
        "            det_box = (det_vehicle[0], det_vehicle[1], det_vehicle[0] + det_vehicle[2], det_vehicle[1] + det_vehicle[3])\n",
        "            if calculate_iou(gt_box, det_box) >= iou_threshold:\n",
        "                matched = True\n",
        "                break\n",
        "\n",
        "        if matched:\n",
        "            true_positives += 1\n",
        "        else:\n",
        "            false_negatives += 1\n",
        "\n",
        "    false_positives = len(detected_vehicles) - true_positives\n",
        "\n",
        "    return true_positives, false_positives, false_negatives\n",
        "\n",
        "def read_ground_truth_from_csv(ground_truth_csv):\n",
        "    \"\"\"\n",
        "    Read the ground truth vehicle locations from a CSV file.\n",
        "    This function returns a dictionary where the keys are the labels and the values are lists of vehicle bounding boxes.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(ground_truth_csv)\n",
        "    if 'labels' not in df.columns:\n",
        "        raise ValueError(\"CSV file must contain a 'labels' column.\")\n",
        "    if 'x_min' not in df.columns or 'y_min' not in df.columns or 'x_max' not in df.columns or 'y_max' not in df.columns:\n",
        "        raise ValueError(\"CSV file must contain 'x_min', 'y_min', 'x_max', and 'y_max' columns.\")\n",
        "\n",
        "    labels = df['labels'].unique()\n",
        "    ground_truth_data = {label: [] for label in labels}\n",
        "    for _, row in df.iterrows():\n",
        "        label = row['labels']\n",
        "        x_min = int(row['x_min'])\n",
        "        y_min = int(row['y_min'])\n",
        "        x_max = int(row['x_max'])\n",
        "        y_max = int(row['y_max'])\n",
        "        box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
        "        ground_truth_data[label].append(box)\n",
        "    return ground_truth_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrQDhnMS3yXH",
        "outputId": "98086f5f-489e-466c-8189-8b77ca0daba8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2024-7-14 Python-3.10.12 torch-2.3.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Load YOLOv5 model from PyTorch Hub\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Initialize tracker\n",
        "class SimpleTracker:\n",
        "    def __init__(self):\n",
        "        self.trackers = []\n",
        "\n",
        "    def update(self, detections):\n",
        "        \"\"\"\n",
        "        Update the tracker with the latest detections.\n",
        "        This function takes a list of detections (bounding boxes) and updates the existing trackers.\n",
        "        It returns the updated list of trackers.\n",
        "        \"\"\"\n",
        "        for tracker in self.trackers:\n",
        "            tracker['lost'] += 1\n",
        "\n",
        "        for detection in detections:\n",
        "            x1, y1, x2, y2, conf, cls = detection\n",
        "            matched = False\n",
        "            for tracker in self.trackers:\n",
        "                if self.iou(tracker['bbox'], [x1, y1, x2, y2]) > 0.5:\n",
        "                    tracker['bbox'] = [x1, y1, x2, y2]\n",
        "                    tracker['lost'] = 0\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                self.trackers.append({'bbox': [x1, y1, x2, y2], 'lost': 0})\n",
        "\n",
        "        self.trackers = [t for t in self.trackers if t['lost'] < 5]\n",
        "\n",
        "        return self.trackers\n",
        "\n",
        "    def iou(self, bbox1, bbox2):\n",
        "        \"\"\"\n",
        "        Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
        "        This function is used to determine if a detection matches an existing tracker.\n",
        "        \"\"\"\n",
        "        x1, y1, x2, y2 = bbox1\n",
        "        x1_, y1_, x2_, y2_ = bbox2\n",
        "\n",
        "        xi1, yi1 = max(x1, x1_), max(y1, y1_)\n",
        "        xi2, yi2 = min(x2, x2_), min(y2, y2_)\n",
        "\n",
        "        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "\n",
        "        bbox1_area = (x2 - x1) * (y2 - y1)\n",
        "        bbox2_area = (x2_ - x1_) * (y2_ - y1_)\n",
        "\n",
        "        union_area = bbox1_area + bbox2_area - inter_area\n",
        "\n",
        "        return inter_area / union_area\n",
        "\n",
        "# Calculate Euclidean distance between two vehicles\n",
        "def calculate_distance(vehicle1, vehicle2):\n",
        "    \"\"\"\n",
        "    Calculate the Euclidean distance between the centers of two vehicles.\n",
        "    This function is used to determine if two vehicles are too close to each other.\n",
        "    \"\"\"\n",
        "    pos1 = np.array(vehicle1['center'])\n",
        "    pos2 = np.array(vehicle2['center'])\n",
        "    distance = np.linalg.norm(pos1 - pos2)\n",
        "    return distance\n",
        "\n",
        "# Process video frames\n",
        "def process_video(video_path):\n",
        "    \"\"\"\n",
        "    Process the input video and detect vehicles, track them, and check for vehicle cut-in warnings.\n",
        "    This function reads the video frames, processes them using the YOLOv5 model and the SimpleTracker,\n",
        "    and writes the output video with the detected vehicles and the warning messages.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    tracker = SimpleTracker()\n",
        "    vehicle_tracks = {}\n",
        "    warning_distance = 50  # Threshold distance in pixels\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter('warnings.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        results = model(frame)\n",
        "        detections = []\n",
        "        for *box, conf, cls in results.xyxy[0].cpu().numpy():\n",
        "            if cls == 2:  # Only consider cars\n",
        "                detections.append(box + [conf, cls])\n",
        "\n",
        "        tracks = tracker.update(detections)\n",
        "\n",
        "        for i, track in enumerate(tracks):\n",
        "            x1, y1, x2, y2 = map(int, track['bbox'])\n",
        "            center = [(x1 + x2) / 2, (y1 + y2) / 2]\n",
        "            track['center'] = center\n",
        "            if i not in vehicle_tracks:\n",
        "                vehicle_tracks[i] = deque(maxlen=5)\n",
        "            vehicle_tracks[i].append(center)\n",
        "            if len(vehicle_tracks[i]) > 1:\n",
        "                track['velocity'] = np.mean(np.diff(vehicle_tracks[i], axis=0), axis=0)\n",
        "            else:\n",
        "                track['velocity'] = np.array([0, 0])\n",
        "\n",
        "            # Draw bounding box and center\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.circle(frame, (int(center[0]), int(center[1])), 2, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f'ID: {i}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        for i, track in enumerate(tracks):\n",
        "            for j, other_track in enumerate(tracks):\n",
        "                if i != j:\n",
        "                    distance = calculate_distance(track, other_track)\n",
        "                    if distance < warning_distance:\n",
        "                        cv2.putText(frame, f'Warning! Vehicles {i} and {j} too close!', (50, 50 + 30*i), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_path ='New_Project.mp4'  # Replace with your local video file path\n",
        "    process_video(video_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}